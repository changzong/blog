---
layout: post
title:  "大数据计算存储与实时计算技术架构"
date:   2016-05-16 09:16:00
categories: jekyll update
---
做了一段时间的大数据开发，写了不少Spark程序和Hive查询，以及Kafka数据获取，现在来总结一下这段时间来的工作。

* 日志的接入

通过在App产品的代码中埋点我们能够获取在各个事件发生时某个用户进行的操作，他的账户状态，他的设备信息，以及所访问的服务端接口信息。这些日志信息都是将来我们进行数据分析和挖掘的宝贵资源。

App的日志通过一台日志服务器（或者一些服务器集群）收集下来后，通过一段Java代码直接发送至Kafka消息服务器，这时候我们希望这些消息能够在有本地备份的同时，也能够继续被实时用来做分析处理，因此这里的流向分为了两部分。一份是通过Flume这个日志采集传输系统写入到Hadoop集群中的HDFS文件系统中。另一份是可以自己编写Spark程序来消费Kafka中的数据做计算处理，之后再以不同的Topic写回Kafka中。

Flume是一个可靠地分布式日志采集聚合传输应用，能够制定数据的上游方式，收集到数据后进行简单的处理再写到指定的接收方，在我们这里，数据的发送方（Source）即Kafka，数据的接收方（Sink）即HDFS，它是实时写入的。

Spark程序同样能够消费Kafka的消息，我们使用了Spark Streaming设定时间间隔流式地从Kafka里读取数据，通过一些解压，聚合，过滤，计算等数据处理方式再将新产生的日志数据重新以Producer的形式写回Kafka中为之后的进一步使用做准备。同时，也可以直接使用Web框架将处理后的数据展现在前端页面。

最近同事们为了实现实时监控指标的功能提出了使用ELK的技术架构，这时，LogStash就代替了Flume成为了数据采集传输工具，其接收端为Elasticsearch，在ES中聚合计算后让Kibana读取生成可视化图表。

* 风控实战

之后我们继续做了以大数据架构为基础的风控项目，目的是能够在较短的时间内即使发现我们的用户的潜在刷单或其他异常行为，方便我们业务团队处理问题。我们最初针对每一个业务板块制定了相应的风险规则，当某一用户在一段时间范围内的某一行为发生了超过阈值的次数时，我们的系统要将他筛选出来制成报表提供给业务团队。

为此我们制定了两套数据流，一套是相对离线的，即按天汇总数据后每天生成之前一周的风险数据。另一种是相对实时的，即按分钟汇总数据后准实时生成可视化图表展示在短时间内超过某阈值的用户行为。第一套基于的数据架构是之前提到的Kafka -> Flume -> HDFS -> MySQL -> RESTful API -> WebApp，数据来源是HDFS上过去一周的日志数据。第二套基于的是Kafka -> Spark -> Kafka -> LogStash -> ES -> Kibana，数据来源直接来自于Kafka。

针对第一套流程，我们每天通过Oozie跑一次Hive脚本将日志数据导入到MySQL中，之后马上再MySQL里跟其他表做关联再聚合生成完整的结果表，这是通过脚本来控制存储过程每天执行的。Web应用默认查找最新的结果表显示出来，也可以向前推找到历史结果，或者下载成Excel格式文件。

针对第二套流程，我们编写了Scala程序，通过Spark Streaming每隔一分钟消费一次Kafka中的数据，解压计算后使用新的topic以生产者发送回Kafka。LogStash从Kafka收集下来日志后存入ES，进行简单的聚合处理成可视化对应的数据结构，为Kibana提供数据。


至此，大数据的技术架构和数据流以及其一次实际应用成功完成。接下来的工作希望能够利用大数据架构和数据挖掘算法进行更深层次的分析用户行为来实施更加有效的线下措施。

下图附上大数据技术架构图：

![BigData](http://7xoylk.com1.z0.glb.clouddn.com/%E5%B9%BB%E7%81%AF%E7%89%871.jpg)
